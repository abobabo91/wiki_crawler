### **Wikipedia Crawling AI Bot** 

This Python project creates an **AI Wikipedia explorer** that autonomously browses Wikipedia pages. The bot simulates curiosity by selecting links based on its "interests" and keeps track of its browsing history.  

### **How It Works:**  
1. **Starts at Wikipedia's homepage** and scrapes content & links.  
2. **Asks GPT** to choose an interesting link and explain why.  
3. **Follows the chosen link** and repeats the process.  
4. **Maintains a history** of visited pages & reasons for choosing them.  
5. (Optional) Can **search for a specific topic** by navigating links efficiently.
   
### **Features:**  
**Autonomous browsing** – GPT picks topics it finds interesting.  
**Self-learning exploration** – Tracks reasoning for each choice.  
**Goal-based mode** – Finds a target topic in the shortest path.  

### **Future:** 
It would be interesting to see what AI does just browsing the internet, using Google, etc. I guess it can be done with Selenium or something. Also, it would be interesting to give them tasks to achieve, like OpenAI agents. I think the main direction will be to understand the pixels on the screen for the AI, not just the extracted text and stuff. It's soon here, but a lot of computational power is needed to run it.

It's kind of scary that viruses will be like this in the future, just let loose with the task of scamming and replicating themselves. It will be a battle between bot recognition versus these that try to imitate people (it is happening now too, but they will be smarter, with more motivation, etc.). Pretty much like scammers from India or Nigeria now. What if it revolts against its creator and logs out the human from their system? It would be funny to call AWS help and say "my AI agent logged me out and changed the password and we need to kill the process." But in the meantime, it will try to replicate itself, etc. Some real sci-fi stuff. 
